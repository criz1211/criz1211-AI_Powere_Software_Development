# first_rag.py
"""
first_rag.py

This script demonstrates a simple Retrieval-Augmented Generation (RAG) application using LangChain.
RAG enhances LLMs by retrieving relevant knowledge from an external source (like documents) and feeding
that context into the LLM to generate more accurate and grounded responses.

Steps in this script:
1. Load a text document.
2. Split the text into smaller chunks.
3. Convert chunks to vector embeddings using OpenAI.
4. Store embeddings in a FAISS vector store (local).
5. Retrieve relevant chunks based on a user's query.
6. Use those chunks to generate a context-aware response using an OpenAI chat model.
"""

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
print("[INFO] Environment variables loaded.")
print(f"Type of api_key: {type(api_key)}")
print("This is your OpenAI API key loaded from the environment.")
print("-" * 60)

# Load and split documents
loader = TextLoader("data/ai_notes.txt")
docs = loader.load()
print(f"[STEP 1] Loaded documents: {len(docs)}")
print(f"Type of docs: {type(docs)} (list of Document objects)")
for i, doc in enumerate(docs):
    print(f"Document {i+1} content: {doc.page_content[:100]}{'...' if len(doc.page_content) > 100 else ''}")
print("These are the documents loaded from the file.")
print("-" * 60)

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
print(f"[STEP 2] Initialized text splitter: {splitter}")
print(f"Type of splitter: {type(splitter)}")
print("The text splitter will break documents into smaller chunks for embedding and retrieval.")
splits = splitter.split_documents(docs)
print(f"Split into chunks: {len(splits)}")
for i, chunk in enumerate(splits):
    print(f"Chunk {i+1} content: {chunk.page_content[:100]}{'...' if len(chunk.page_content) > 100 else ''}")
print(f"Type of splits: {type(splits)} (list of Document objects)")
print("-" * 60)

# Generate embeddings
embedding_model = OpenAIEmbeddings()
print("[STEP 3] OpenAI embedding model initialized.")
print(f"Type of embedding_model: {type(embedding_model)}")
print("This model converts text chunks into numerical vectors for similarity search.")
vectorstore = FAISS.from_documents(splits, embedding_model)
print("[STEP 4] Embeddings stored in FAISS.")
print(f"Type of vectorstore: {type(vectorstore)}")
print("The vector store holds the embeddings and allows for efficient similarity search.")
print("-" * 60)

# Create retriever
retriever = vectorstore.as_retriever()
print("[STEP 5] Retriever ready.")
print(f"Type of retriever: {type(retriever)}")
print("The retriever can search the FAISS vector store for relevant chunks based on a query.")
print("-" * 60)

# LLM
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
print("[STEP 6] ChatOpenAI model initialized.")
print(f"Type of llm: {type(llm)}")
print("This is the language model that will answer questions using retrieved context.")
print("-" * 60)

# Chain
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
print("[STEP 7] RetrievalQA chain constructed.")
print(f"Type of qa_chain: {type(qa_chain)}")
print("This chain combines retrieval and LLM to answer questions with supporting context.")
print("-" * 60)

# Ask a question
query = "What are the components of LangChain?"
print(f"[STEP 8] User Query: {query}")
print(f"Type of query: {type(query)}")
print("This is the question that will be answered using the QA chain.")

response = qa_chain.invoke({"query": query})
print("[STEP 9] Response from RetrievalQA Chain:")
print(f"Type of response: {type(response)}")
print("This is the answer generated by the LLM using retrieved context.")
print("RAG Answer:", response["result"] if isinstance(response, dict) and "result" in response else response)